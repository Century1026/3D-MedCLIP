{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8381e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.6,>=1.22.4 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 2))\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting Pillow (from -r requirements.txt (line 3))\n",
      "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting requests (from -r requirements.txt (line 4))\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 5))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting wget (from -r requirements.txt (line 6))\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk>=3.7 (from -r requirements.txt (line 7))\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting scikit_learn>=1.1.2 (from -r requirements.txt (line 8))\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting textaugment>=1.3.4 (from -r requirements.txt (line 9))\n",
      "  Using cached textaugment-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting timm>=0.6.11 (from -r requirements.txt (line 10))\n",
      "  Using cached timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting torch>=1.12.1 (from -r requirements.txt (line 12))\n",
      "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision>=0.13.1 (from -r requirements.txt (line 13))\n",
      "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting scipy<1.17,>=1.10.0 (from -r requirements.txt (line 15))\n",
      "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting transformers<=4.24.0,>=4.23.1 (from -r requirements.txt (line 16))\n",
      "  Using cached transformers-4.24.0-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting nibabel>=3.2.0 (from -r requirements.txt (line 17))\n",
      "  Using cached nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0 (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Downloading huggingface_hub-0.35.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16)) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.10.0->transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16)) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.10.0->transformers<=4.24.0,>=4.23.1->-r requirements.txt (line 16))\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 2))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 2))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->-r requirements.txt (line 4))\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->-r requirements.txt (line 4))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->-r requirements.txt (line 4))\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->-r requirements.txt (line 4))\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting click (from nltk>=3.7->-r requirements.txt (line 7))\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>=3.7->-r requirements.txt (line 7))\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit_learn>=1.1.2->-r requirements.txt (line 8))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gensim>=4.0 (from textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting textblob (from textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting googletrans>=2 (from textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting safetensors (from timm>=0.6.11->-r requirements.txt (line 10))\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.12.1->-r requirements.txt (line 12)) (78.1.1)\n",
      "Collecting importlib-resources>=5.12 (from nibabel>=3.2.0->-r requirements.txt (line 17))\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting numpy<2.6,>=1.22.4 (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.17,>=1.10.0 (from -r requirements.txt (line 15))\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim>=4.0->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting httpx>=0.27.2 (from httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting anyio (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached h2-4.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting hyperframe<7,>=6.1 (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached hyperframe-6.1.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting hpack<5,>=4.1 (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached hpack-4.1.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim>=4.0->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans>=2->textaugment>=1.3.4->-r requirements.txt (line 9))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.12.1->-r requirements.txt (line 12))\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Using cached transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading huggingface_hub-0.35.0-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.4/563.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (150 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached textaugment-2.0.0-py3-none-any.whl (19 kB)\n",
      "Using cached timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached h2-4.3.0-py3-none-any.whl (61 kB)\n",
      "Using cached hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Using cached hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (82 kB)\n",
      "Building wheels for collected packages: wget\n",
      "\u001b[33m  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9687 sha256=432da7b532c0481259118548a1a7f58947fe8f7f9a8ad788ce520dcf9cdb3c51\n",
      "  Stored in directory: /home/Student2025/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, tokenizers, pytz, nvidia-cusparselt-cu12, mpmath, wrapt, urllib3, tzdata, triton, tqdm, threadpoolctl, sympy, sniffio, safetensors, regex, pyyaml, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, importlib-resources, idna, hyperframe, hpack, hf-xet, h11, fsspec, filelock, click, charset_normalizer, certifi, smart-open, scipy, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nltk, nibabel, jinja2, httpcore, h2, anyio, textblob, scikit_learn, nvidia-cusolver-cu12, huggingface-hub, httpx, gensim, transformers, torch, torchvision, googletrans, timm, textaugment\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66/66\u001b[0m [textaugment]\u001b[0m [timm]vision]]ub]cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.3.0 anyio-4.10.0 certifi-2025.8.3 charset_normalizer-3.4.3 click-8.2.1 filelock-3.19.1 fsspec-2025.9.0 gensim-4.3.3 googletrans-4.0.2 h11-0.16.0 h2-4.3.0 hf-xet-1.1.10 hpack-4.1.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.0 hyperframe-6.1.0 idna-3.10 importlib-resources-6.5.2 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 nibabel-5.3.2 nltk-3.9.1 numpy-1.26.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pandas-2.3.2 pytz-2025.2 pyyaml-6.0.2 regex-2025.9.1 requests-2.32.5 safetensors-0.6.2 scikit_learn-1.7.2 scipy-1.13.1 smart-open-7.3.1 sniffio-1.3.1 sympy-1.14.0 textaugment-2.0.0 textblob-0.19.0 threadpoolctl-3.6.0 timm-1.0.19 tokenizers-0.13.3 torch-2.8.0 torchvision-0.23.0 tqdm-4.67.1 transformers-4.24.0 triton-3.4.0 tzdata-2025.2 urllib3-2.5.0 wget-3.2 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64228e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mnt/HDD16TB/Human_MRI/ADNI_Multimodel/data/Step1_3DDeepC_T1_FS_Preprocessed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d50aaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/1 | Step 0 | Loss: 1.0128\n",
      "Epoch 1/1 | Step 10 | Loss: 0.9402\n",
      "Epoch 1/1 | Step 20 | Loss: 1.0120\n",
      "Epoch 1/1 | Step 30 | Loss: 1.0490\n",
      "Epoch 1/1 | Step 40 | Loss: 1.0390\n",
      "Epoch 1/1 | Step 50 | Loss: 0.9979\n",
      "Epoch 1/1 | Step 60 | Loss: 0.9022\n",
      "Epoch 1/1 | Step 70 | Loss: 1.1316\n",
      "Epoch 1/1 | Step 80 | Loss: 0.9928\n",
      "Epoch 1/1 | Step 90 | Loss: 0.7907\n",
      "Epoch 1/1 | Step 100 | Loss: 0.8012\n",
      "Epoch 1/1 | Step 110 | Loss: 0.5487\n",
      "Epoch 1/1 | Step 120 | Loss: 0.6839\n",
      "Epoch 1/1 | Step 130 | Loss: 0.5968\n",
      "Epoch 1/1 | Step 140 | Loss: 0.5529\n",
      "Epoch 1/1 | Step 150 | Loss: 0.7259\n",
      "Epoch 1/1 | Step 160 | Loss: 0.6435\n",
      "Epoch 1/1 | Step 170 | Loss: 0.6123\n",
      "Epoch 1/1 | Step 180 | Loss: 0.7101\n",
      "Epoch 1/1 | Step 190 | Loss: 0.5730\n",
      "Epoch 1/1 | Step 200 | Loss: 0.6236\n",
      "Epoch 1/1 | Step 210 | Loss: 0.5677\n",
      "Epoch 1/1 | Step 220 | Loss: 0.4687\n",
      "Epoch 1/1 | Step 230 | Loss: 0.6267\n",
      "Epoch 1/1 | Step 240 | Loss: 0.4798\n",
      "Epoch 1/1 | Step 250 | Loss: 0.5126\n",
      "Epoch 1/1 | Step 260 | Loss: 0.4839\n",
      "Epoch 1/1 | Step 270 | Loss: 0.5624\n",
      "Epoch 1/1 | Step 280 | Loss: 0.5364\n",
      "Epoch 1/1 | Step 290 | Loss: 0.9900\n",
      "Epoch 1/1 | Step 300 | Loss: 0.5372\n",
      "Epoch 1/1 | Step 310 | Loss: 0.5387\n",
      "Epoch 1/1 | Step 320 | Loss: 0.5081\n",
      "Epoch 1/1 | Step 330 | Loss: 0.4565\n",
      "Epoch 1/1 | Step 340 | Loss: 0.3835\n",
      "Epoch 1/1 | Step 350 | Loss: 0.5138\n",
      "Epoch 1/1 | Step 360 | Loss: 0.3975\n",
      "Epoch 1/1 | Step 370 | Loss: 0.3588\n",
      "Epoch 1/1 | Step 380 | Loss: 0.5704\n",
      "Epoch 1/1 | Step 390 | Loss: 0.4909\n",
      "Epoch 1/1 | Step 400 | Loss: 0.6267\n",
      "Epoch 1/1 | Step 410 | Loss: 0.4683\n",
      "Epoch 1/1 | Step 420 | Loss: 0.5419\n",
      "Epoch 1/1 | Step 430 | Loss: 0.3765\n",
      "Epoch 1/1 | Step 440 | Loss: 0.4226\n",
      "Epoch 1/1 | Step 450 | Loss: 0.5516\n",
      "Epoch 1/1 | Step 460 | Loss: 0.4588\n",
      "Epoch 1/1 | Step 470 | Loss: 0.6778\n",
      "Epoch 1/1 | Step 480 | Loss: 0.4432\n",
      "Epoch 1/1 | Step 490 | Loss: 0.4643\n",
      "Epoch 1/1 | Step 500 | Loss: 0.4167\n",
      "Epoch 1/1 | Step 510 | Loss: 0.4297\n",
      "Epoch 1/1 | Step 520 | Loss: 0.4545\n",
      "Epoch 1/1 | Step 530 | Loss: 0.3764\n",
      "Epoch 1/1 | Step 540 | Loss: 0.3933\n",
      "Epoch 1/1 | Step 550 | Loss: 0.4541\n",
      "Epoch 1/1 | Step 560 | Loss: 0.5663\n",
      "Epoch 1/1 | Step 570 | Loss: 0.4538\n",
      "Epoch 1/1 | Step 580 | Loss: 0.4050\n",
      "Epoch 1/1 | Step 590 | Loss: 0.4775\n",
      "Epoch 1/1 | Step 600 | Loss: 0.3617\n",
      "Epoch 1/1 | Step 610 | Loss: 0.4342\n",
      "Epoch 1/1 | Step 620 | Loss: 0.6848\n",
      "Epoch 1/1 | Step 630 | Loss: 0.4346\n",
      "Epoch 1/1 | Step 640 | Loss: 0.4557\n",
      "Epoch 1/1 | Step 650 | Loss: 0.3697\n",
      "Epoch 1/1 | Step 660 | Loss: 0.4422\n",
      "Epoch 1/1 | Step 670 | Loss: 0.3431\n",
      "Epoch 1/1 | Step 680 | Loss: 0.4120\n",
      "Epoch 1/1 | Step 690 | Loss: 0.3285\n",
      "Epoch 1/1 | Step 700 | Loss: 0.4156\n",
      "Epoch 1/1 | Step 710 | Loss: 0.2720\n",
      "Epoch 1/1 | Step 720 | Loss: 0.4731\n",
      "Epoch 1/1 | Step 730 | Loss: 0.3074\n",
      "Epoch 1/1 | Step 740 | Loss: 0.2964\n",
      "Epoch 1/1 | Step 750 | Loss: 0.3364\n",
      "Epoch 1/1 | Step 760 | Loss: 0.4178\n",
      "Epoch 1/1 | Step 770 | Loss: 0.3277\n",
      "Epoch 1/1 | Step 780 | Loss: 0.3095\n",
      "Epoch 1/1 | Step 790 | Loss: 0.3532\n",
      "Epoch 1/1 | Step 800 | Loss: 0.5121\n",
      "Epoch 1/1 | Step 810 | Loss: 0.2401\n",
      "Epoch 1/1 | Step 820 | Loss: 0.3182\n",
      "Epoch 1/1 | Step 830 | Loss: 0.3689\n",
      "Epoch 1/1 | Step 840 | Loss: 0.3187\n",
      "Epoch 1/1 | Step 850 | Loss: 0.3210\n",
      "Epoch 1/1 | Step 860 | Loss: 0.3444\n",
      "Epoch 1/1 | Step 870 | Loss: 0.3435\n",
      "Epoch 1/1 | Step 880 | Loss: 0.2713\n",
      "Epoch 1/1 | Step 890 | Loss: 0.3067\n",
      "Epoch 1/1 | Step 900 | Loss: 0.2620\n",
      "Epoch 1/1 | Step 910 | Loss: 0.3564\n",
      "Epoch 1/1 | Step 920 | Loss: 0.3514\n",
      "Epoch 1/1 | Step 930 | Loss: 0.3810\n",
      "Epoch 1/1 | Step 940 | Loss: 0.3166\n",
      "Epoch 1/1 | Step 950 | Loss: 0.3594\n",
      "Epoch 1/1 | Step 960 | Loss: 0.2210\n",
      "Epoch 1/1 | Step 970 | Loss: 0.2624\n",
      "Epoch 1/1 | Step 980 | Loss: 0.3730\n",
      "Epoch 1/1 | Step 990 | Loss: 0.2888\n",
      "Epoch 1/1 | Step 1000 | Loss: 0.3847\n",
      "Epoch 1/1 | Step 1010 | Loss: 0.2642\n",
      "Epoch 1/1 | Step 1020 | Loss: 0.4362\n",
      "Epoch 1/1 | Step 1030 | Loss: 0.3241\n",
      "Epoch 1/1 | Step 1040 | Loss: 0.2633\n",
      "Epoch 1/1 | Step 1050 | Loss: 0.3630\n",
      "Epoch 1/1 | Step 1060 | Loss: 0.3070\n",
      "Epoch 1/1 | Step 1070 | Loss: 0.2392\n",
      "Epoch 1/1 | Step 1080 | Loss: 0.3710\n",
      "Epoch 1/1 | Step 1090 | Loss: 0.3040\n",
      "Epoch 1/1 | Step 1100 | Loss: 0.2968\n",
      "Epoch 1/1 | Step 1110 | Loss: 0.2881\n",
      "Epoch 1/1 | Step 1120 | Loss: 0.3113\n",
      "Epoch 1/1 | Step 1130 | Loss: 0.4108\n",
      "Epoch 1/1 | Step 1140 | Loss: 0.4085\n",
      "Epoch 1/1 | Step 1150 | Loss: 0.3221\n",
      "Epoch 1/1 | Step 1160 | Loss: 0.2811\n",
      "Epoch 1/1 | Step 1170 | Loss: 0.3250\n",
      "Epoch 1/1 | Step 1180 | Loss: 0.2590\n",
      "Epoch 1/1 | Step 1190 | Loss: 0.3173\n",
      "Epoch 1/1 | Step 1200 | Loss: 0.2890\n",
      "Epoch 1/1 | Step 1210 | Loss: 0.3434\n",
      "Epoch 1/1 | Step 1220 | Loss: 0.4027\n",
      "Epoch 1/1 | Step 1230 | Loss: 0.2432\n",
      "Epoch 1/1 | Step 1240 | Loss: 0.2693\n",
      "Epoch 1/1 | Step 1250 | Loss: 0.2179\n",
      "Epoch 1/1 | Step 1260 | Loss: 0.2601\n",
      "Epoch 1/1 | Step 1270 | Loss: 0.3138\n",
      "Epoch 1/1 | Step 1280 | Loss: 0.2779\n",
      "Epoch 1/1 | Step 1290 | Loss: 0.3241\n",
      "Epoch 1/1 | Step 1300 | Loss: 0.5485\n",
      "Epoch 1/1 | Step 1310 | Loss: 0.3467\n",
      "Epoch 1/1 | Step 1320 | Loss: 0.3272\n",
      "Epoch 1/1 | Step 1330 | Loss: 0.3561\n",
      "Epoch 1/1 | Step 1340 | Loss: 0.3329\n",
      "Epoch 1/1 | Step 1350 | Loss: 0.2034\n",
      "Epoch 1/1 | Step 1360 | Loss: 0.2617\n",
      "Epoch 1/1 | Step 1370 | Loss: 0.3923\n",
      "Epoch 1/1 | Step 1380 | Loss: 0.3185\n",
      "Epoch 1/1 | Step 1390 | Loss: 0.3352\n",
      "Epoch 1/1 | Step 1400 | Loss: 0.4478\n",
      "Epoch 1/1 | Step 1410 | Loss: 0.2688\n",
      "Epoch 1/1 | Step 1420 | Loss: 0.2982\n",
      "Epoch 1/1 | Step 1430 | Loss: 0.2395\n",
      "Epoch 1/1 | Step 1440 | Loss: 0.2487\n",
      "Epoch 1/1 | Step 1450 | Loss: 0.2912\n",
      "Epoch 1/1 | Step 1460 | Loss: 0.3156\n",
      "Epoch 1/1 | Step 1470 | Loss: 0.3815\n",
      "Epoch 1/1 | Step 1480 | Loss: 0.3916\n",
      "Epoch 1/1 | Step 1490 | Loss: 0.2201\n",
      "Epoch 1/1 | Step 1500 | Loss: 0.2734\n",
      "Epoch 1/1 | Step 1510 | Loss: 0.3090\n",
      "Epoch 1/1 | Step 1520 | Loss: 0.3784\n",
      "Epoch 1/1 | Step 1530 | Loss: 0.3100\n",
      "Epoch 1/1 | Step 1540 | Loss: 0.3207\n",
      "Epoch 1/1 | Step 1550 | Loss: 0.4056\n",
      "Epoch 1/1 | Step 1560 | Loss: 0.2829\n",
      "Epoch 1/1 | Step 1570 | Loss: 0.3833\n",
      "Epoch 1/1 | Step 1580 | Loss: 0.2199\n",
      "Epoch 1/1 | Step 1590 | Loss: 0.2741\n",
      "Epoch 1/1 | Step 1600 | Loss: 0.3714\n",
      "Epoch 1/1 | Step 1610 | Loss: 0.2808\n",
      "Epoch 1/1 | Step 1620 | Loss: 0.2727\n",
      "Epoch 1/1 | Step 1630 | Loss: 0.3500\n",
      "Epoch 1/1 | Step 1640 | Loss: 0.2989\n",
      "Epoch 1/1 | Step 1650 | Loss: 0.4113\n",
      "Epoch 1/1 | Step 1660 | Loss: 0.3141\n",
      "Epoch 1/1 | Step 1670 | Loss: 0.2264\n",
      "Epoch 1/1 | Step 1680 | Loss: 0.3649\n",
      "Epoch 1/1 | Step 1690 | Loss: 0.2122\n",
      "Epoch 1/1 | Step 1700 | Loss: 0.3732\n",
      "Epoch 1/1 | Step 1710 | Loss: 0.3777\n",
      "Epoch 1/1 | Step 1720 | Loss: 0.2352\n",
      "Epoch 1/1 | Step 1730 | Loss: 0.2081\n",
      "Epoch 1/1 | Step 1740 | Loss: 0.2522\n",
      "Epoch 1/1 | Step 1750 | Loss: 0.2995\n",
      "Epoch 1/1 | Step 1760 | Loss: 0.2699\n",
      "Epoch 1/1 | Step 1770 | Loss: 0.3670\n",
      "Epoch 1/1 | Step 1780 | Loss: 0.2605\n",
      "Epoch 1/1 | Step 1790 | Loss: 0.5124\n",
      "Epoch 1/1 | Step 1800 | Loss: 0.2722\n",
      "Epoch 1/1 | Step 1810 | Loss: 0.3547\n",
      "Epoch 1/1 | Step 1820 | Loss: 0.3229\n",
      "Epoch 1/1 | Step 1830 | Loss: 0.2975\n",
      "Epoch 1/1 | Step 1840 | Loss: 0.1946\n",
      "Epoch 1/1 | Step 1850 | Loss: 0.2648\n",
      "Epoch 1/1 | Step 1860 | Loss: 0.3561\n",
      "Epoch 1/1 | Step 1870 | Loss: 0.2700\n",
      "Epoch 1/1 | Step 1880 | Loss: 0.3087\n",
      "Epoch 1/1 | Step 1890 | Loss: 0.4624\n",
      "Epoch 1/1 | Step 1900 | Loss: 0.2928\n",
      "Epoch 1/1 | Step 1910 | Loss: 0.2858\n",
      "Epoch 1/1 | Step 1920 | Loss: 0.2136\n",
      "Epoch 1/1 | Step 1930 | Loss: 0.2297\n",
      "Epoch 1/1 | Step 1940 | Loss: 0.3182\n",
      "Epoch 1/1 | Step 1950 | Loss: 0.3518\n",
      "Epoch 1/1 | Step 1960 | Loss: 0.2397\n",
      "Epoch 1/1 | Step 1970 | Loss: 0.3093\n",
      "Epoch 1/1 | Step 1980 | Loss: 0.3930\n",
      "Epoch 1/1 | Step 1990 | Loss: 0.2655\n",
      "Epoch 1/1 | Step 2000 | Loss: 0.3104\n",
      "Epoch 1/1 | Step 2010 | Loss: 0.1953\n",
      "Epoch 1/1 | Step 2020 | Loss: 0.3526\n",
      "Epoch 1/1 | Step 2030 | Loss: 0.3152\n",
      "Epoch 1/1 | Step 2040 | Loss: 0.3099\n",
      "Epoch 1/1 | Step 2050 | Loss: 0.3340\n",
      "Epoch 1/1 | Step 2060 | Loss: 0.2590\n",
      "Epoch 1/1 | Step 2070 | Loss: 0.3059\n",
      "Epoch 1/1 | Step 2080 | Loss: 0.3332\n",
      "Epoch 1/1 | Step 2090 | Loss: 0.3418\n",
      "Epoch 1/1 | Step 2100 | Loss: 0.2632\n",
      "Epoch 1/1 | Step 2110 | Loss: 0.1817\n",
      "Epoch 1/1 | Step 2120 | Loss: 0.2812\n",
      "Epoch 1/1 | Step 2130 | Loss: 0.2139\n",
      "Epoch 1/1 | Step 2140 | Loss: 0.4840\n",
      "Epoch 1/1 | Step 2150 | Loss: 0.2485\n",
      "Epoch 1/1 | Step 2160 | Loss: 0.2509\n",
      "Epoch 1/1 | Step 2170 | Loss: 0.2334\n",
      "Epoch 1/1 | Step 2180 | Loss: 0.2544\n",
      "Epoch 1/1 | Step 2190 | Loss: 0.2059\n",
      "Epoch 1/1 | Step 2200 | Loss: 0.3703\n",
      "Epoch 1/1 | Step 2210 | Loss: 0.2999\n",
      "Epoch 1/1 | Step 2220 | Loss: 0.4813\n",
      "Epoch 1/1 | Step 2230 | Loss: 0.3122\n",
      "Epoch 1/1 | Step 2240 | Loss: 0.2639\n",
      "Epoch 1/1 | Step 2250 | Loss: 0.2167\n",
      "Epoch 1/1 | Step 2260 | Loss: 0.3007\n",
      "Epoch 1/1 | Step 2270 | Loss: 0.2313\n",
      "Epoch 1/1 | Step 2280 | Loss: 0.1857\n",
      "Epoch 1/1 | Step 2290 | Loss: 0.3436\n",
      "Epoch 1/1 | Step 2300 | Loss: 0.2790\n",
      "Epoch 1/1 | Step 2310 | Loss: 0.2876\n",
      "Epoch 1/1 | Step 2320 | Loss: 0.2303\n",
      "Epoch 1/1 | Step 2330 | Loss: 0.2252\n",
      "Epoch 1/1 | Step 2340 | Loss: 0.4989\n",
      "Epoch 1/1 | Step 2350 | Loss: 0.4946\n",
      "Epoch 1/1 | Step 2360 | Loss: 0.2372\n",
      "Epoch 1/1 | Step 2370 | Loss: 0.4113\n",
      "Epoch 1/1 | Step 2380 | Loss: 0.2250\n",
      "Epoch 1/1 | Step 2390 | Loss: 0.2213\n",
      "Epoch 1/1 | Step 2400 | Loss: 0.2777\n",
      "Epoch 1/1 | Step 2410 | Loss: 0.2336\n",
      "Epoch 1/1 | Step 2420 | Loss: 0.2900\n",
      "Epoch 1/1 | Step 2430 | Loss: 0.2875\n",
      "Epoch 1/1 | Step 2440 | Loss: 0.2269\n",
      "Epoch 1/1 | Step 2450 | Loss: 0.3169\n",
      "Epoch 1/1 | Step 2460 | Loss: 0.1628\n",
      "Epoch 1/1 | Step 2470 | Loss: 0.3427\n",
      "Epoch 1/1 | Step 2480 | Loss: 0.4618\n",
      "Epoch 1/1 | Step 2490 | Loss: 0.2293\n",
      "Epoch 1/1 | Step 2500 | Loss: 0.2202\n",
      "Epoch 1/1 | Step 2510 | Loss: 0.2655\n",
      "Epoch 1/1 | Step 2520 | Loss: 0.1923\n",
      "Epoch 1/1 | Step 2530 | Loss: 0.3012\n",
      "Epoch 1/1 | Step 2540 | Loss: 0.2972\n",
      "Epoch 1/1 | Step 2550 | Loss: 0.3006\n",
      "Epoch 1/1 | Step 2560 | Loss: 0.2281\n",
      "Epoch 1/1 | Step 2570 | Loss: 0.2832\n",
      "Epoch 1/1 | Step 2580 | Loss: 0.2804\n",
      "Epoch 1/1 | Step 2590 | Loss: 0.4121\n",
      "Epoch 1/1 | Step 2600 | Loss: 0.2420\n",
      "Epoch 1/1 | Step 2610 | Loss: 0.2135\n",
      "Epoch 1/1 | Step 2620 | Loss: 0.2274\n",
      "Epoch 1/1 | Step 2630 | Loss: 0.3395\n",
      "Epoch 1/1 | Step 2640 | Loss: 0.2653\n",
      "Epoch 1/1 | Step 2650 | Loss: 0.2099\n",
      "Epoch 1/1 | Step 2660 | Loss: 0.2461\n",
      "Epoch 1/1 | Step 2670 | Loss: 0.2599\n",
      "Epoch 1/1 | Step 2680 | Loss: 0.3964\n",
      "Epoch 1/1 | Step 2690 | Loss: 0.3825\n",
      "Epoch 1/1 | Step 2700 | Loss: 0.2961\n",
      "Epoch 1/1 | Step 2710 | Loss: 0.4500\n",
      "Epoch 1/1 | Step 2720 | Loss: 0.2973\n",
      "Epoch 1/1 | Step 2730 | Loss: 0.4666\n",
      "Epoch 1/1 | Step 2740 | Loss: 0.3096\n",
      "Epoch 1/1 | Step 2750 | Loss: 0.2920\n",
      "Epoch 1/1 | Step 2760 | Loss: 0.2997\n",
      "Epoch 1/1 | Step 2770 | Loss: 0.2236\n",
      "Epoch 1/1 | Step 2780 | Loss: 0.2225\n",
      "Epoch 1/1 | Step 2790 | Loss: 0.2786\n",
      "Epoch 1/1 | Step 2800 | Loss: 0.2212\n",
      "Epoch 1/1 | Step 2810 | Loss: 0.5529\n",
      "Epoch 1/1 | Step 2820 | Loss: 0.3176\n",
      "Epoch 1/1 | Step 2830 | Loss: 0.2131\n",
      "Epoch 1/1 | Step 2840 | Loss: 0.3157\n",
      "Epoch 1/1 | Step 2850 | Loss: 0.1984\n",
      "Epoch 1/1 | Step 2860 | Loss: 0.2996\n",
      "Epoch 1/1 | Step 2870 | Loss: 0.5549\n",
      "Epoch 1/1 | Step 2880 | Loss: 0.3287\n",
      "Epoch 1/1 | Step 2890 | Loss: 0.2327\n",
      "Epoch 1/1 | Step 2900 | Loss: 0.2471\n",
      "Epoch 1/1 | Step 2910 | Loss: 0.2369\n",
      "Epoch 1/1 | Step 2920 | Loss: 0.1902\n",
      "Epoch 1/1 | Step 2930 | Loss: 0.2568\n",
      "Epoch 1/1 | Step 2940 | Loss: 0.3266\n",
      "Epoch 1/1 | Step 2950 | Loss: 0.2341\n",
      "Epoch 1/1 | Step 2960 | Loss: 0.3359\n",
      "Epoch 1/1 | Step 2970 | Loss: 0.2688\n",
      "Epoch 1/1 | Step 2980 | Loss: 0.1976\n",
      "Epoch 1/1 | Step 2990 | Loss: 0.3482\n",
      "Epoch 1/1 | Step 3000 | Loss: 0.1455\n",
      "Epoch 1/1 | Step 3010 | Loss: 0.3169\n",
      "Epoch 1/1 | Step 3020 | Loss: 0.2674\n",
      "Epoch 1/1 | Step 3030 | Loss: 0.2955\n",
      "Epoch 1/1 | Step 3040 | Loss: 0.4216\n",
      "Epoch 1/1 | Step 3050 | Loss: 0.4079\n",
      "Epoch 1/1 | Step 3060 | Loss: 0.3464\n",
      "Epoch 1/1 | Step 3070 | Loss: 0.2128\n",
      "Epoch 1/1 | Step 3080 | Loss: 0.2888\n",
      "Epoch 1/1 | Step 3090 | Loss: 0.2533\n",
      "Epoch 1/1 | Step 3100 | Loss: 0.1851\n",
      "Epoch 1/1 | Step 3110 | Loss: 0.2832\n",
      "Epoch 1/1 | Step 3120 | Loss: 0.2507\n",
      "Epoch 1/1 | Step 3130 | Loss: 0.3350\n",
      "Epoch 1/1 | Step 3140 | Loss: 0.2929\n",
      "Epoch 1/1 | Step 3150 | Loss: 0.2659\n",
      "Epoch 1/1 | Step 3160 | Loss: 0.2443\n",
      "Epoch 1/1 | Step 3170 | Loss: 0.4392\n",
      "Epoch 1/1 | Step 3180 | Loss: 0.2708\n",
      "Epoch 1/1 | Step 3190 | Loss: 0.4696\n",
      "Epoch 1/1 | Step 3200 | Loss: 0.1848\n",
      "Epoch 1/1 | Step 3210 | Loss: 0.2222\n",
      "Epoch 1/1 | Step 3220 | Loss: 0.2582\n",
      "Epoch 1/1 | Step 3230 | Loss: 0.2257\n",
      "Epoch 1/1 | Step 3240 | Loss: 0.2520\n",
      "Epoch 1/1 | Step 3250 | Loss: 0.3230\n",
      "Epoch 1/1 | Step 3260 | Loss: 0.3287\n",
      "Epoch 1/1 | Step 3270 | Loss: 0.2120\n",
      "Epoch 1/1 | Step 3280 | Loss: 0.2218\n",
      "Epoch 1/1 | Step 3290 | Loss: 0.2146\n",
      "Epoch 1/1 | Step 3300 | Loss: 0.2515\n",
      "Epoch 1/1 | Step 3310 | Loss: 0.2448\n",
      "Epoch 1/1 | Step 3320 | Loss: 0.3105\n",
      "Epoch 1/1 | Step 3330 | Loss: 0.2831\n",
      "Epoch 1/1 | Step 3340 | Loss: 0.3341\n",
      "Epoch 1/1 | Step 3350 | Loss: 0.2160\n",
      "Epoch 1/1 | Step 3360 | Loss: 0.2508\n",
      "Epoch 1/1 | Step 3370 | Loss: 0.2792\n",
      "Epoch 1/1 | Step 3380 | Loss: 0.2049\n",
      "Epoch 1/1 | Step 3390 | Loss: 0.3549\n",
      "Epoch 1/1 | Step 3400 | Loss: 0.3348\n",
      "Epoch 1/1 | Step 3410 | Loss: 0.2468\n",
      "Epoch 1/1 | Step 3420 | Loss: 0.3175\n",
      "Epoch 1/1 | Step 3430 | Loss: 0.2126\n",
      "Epoch 1/1 | Step 3440 | Loss: 0.2336\n",
      "Epoch 1/1 | Step 3450 | Loss: 0.3027\n",
      "Epoch 1/1 | Step 3460 | Loss: 0.2320\n",
      "Epoch 1/1 | Step 3470 | Loss: 0.2300\n",
      "Epoch 1/1 | Step 3480 | Loss: 0.4111\n",
      "Epoch 1/1 | Step 3490 | Loss: 0.4325\n",
      "Epoch 1/1 | Step 3500 | Loss: 0.2756\n",
      "Epoch 1/1 | Step 3510 | Loss: 0.2689\n",
      "Epoch 1/1 | Step 3520 | Loss: 0.2464\n",
      "Epoch 1/1 | Step 3530 | Loss: 0.3752\n",
      "Epoch 1/1 | Step 3540 | Loss: 0.2498\n",
      "Epoch 1/1 | Step 3550 | Loss: 0.2600\n",
      "Epoch 1/1 | Step 3560 | Loss: 0.3523\n",
      "Epoch 1/1 | Step 3570 | Loss: 0.2511\n",
      "Epoch 1/1 | Step 3580 | Loss: 0.3049\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Saved full model to ./outputs/mri_pretrain/self_supervised_model.pt and encoder weights to ./outputs/mri_pretrain/encoder.pt.\n",
      "Load the encoder weights into your downstream model with `load_state_dict` for fine-tuning.\n",
      "Training finished. Checkpoints written to: ./outputs/mri_pretrain\n"
     ]
    }
   ],
   "source": [
    "# 3D MRI self-supervised pretraining (uses `data_dir` defined above)\n",
    "from medclip.mri_self_supervised import SelfSupervisedConfig, SelfSupervisedPretrainer\n",
    "\n",
    "# Configure output directory for checkpoints\n",
    "output_dir = './outputs/mri_pretrain'\n",
    "\n",
    "config = SelfSupervisedConfig(\n",
    "    data_source=data_dir,          # your NIfTI folder or a text file of paths\n",
    "    output_dir=output_dir,         # where to save checkpoints and encoder weights\n",
    "    mode='mae',                    # 'mae' or 'contrastive'\n",
    "    volume_size=(96, 96, 96),      # D, H, W\n",
    "    patch_size=(16, 16, 16),       # must divide volume_size\n",
    "    batch_size=2,\n",
    "    epochs=1,                      # increase for real training\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    mask_ratio_range=(0.5, 0.7),   # MAE only; ignored for contrastive\n",
    "    temperature=0.07,              # contrastive only; ignored for MAE\n",
    "    projection_dim=128,            # contrastive only; ignored for MAE\n",
    "    log_every=10,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = SelfSupervisedPretrainer(config)\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Training finished. Checkpoints written to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204dbfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/Student2025/miniconda3/envs/medclip-3.11/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Download pretrained model from: https://storage.googleapis.com/pytrial/medclip-vit-pretrained.zip\n",
      "load model weight from: ./pretrained/medclip-vit\n",
      "dict_keys(['img_embeds', 'text_embeds', 'logits', 'loss_value', 'logits_per_text'])\n"
     ]
    }
   ],
   "source": [
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "from medclip import MedCLIPProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# prepare for the demo image and texts\n",
    "processor = MedCLIPProcessor()\n",
    "image = Image.open('./example_data/view1_frontal.jpg')\n",
    "inputs = processor(\n",
    "    text=[\"lungs remain severely hyperinflated with upper lobe emphysema\", \n",
    "        \"opacity left costophrenic angle is new since prior exam ___ represent some loculated fluid cavitation unlikely\"], \n",
    "    images=image, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    "    )\n",
    "\n",
    "# pass to MedCLIP model\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained()\n",
    "model.cuda()\n",
    "outputs = model(**inputs)\n",
    "print(outputs.keys())\n",
    "# dict_keys(['img_embeds', 'text_embeds', 'logits', 'loss_value', 'logits_per_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe962d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model weight from: ./pretrained/medclip-vit\n",
      "sample 10 num of prompts for Atelectasis from total 210\n",
      "sample 10 num of prompts for Cardiomegaly from total 15\n",
      "sample 10 num of prompts for Consolidation from total 192\n",
      "sample 10 num of prompts for Edema from total 18\n",
      "sample 10 num of prompts for Pleural Effusion from total 54\n",
      "{'logits': tensor([[0.3598, 0.4742, 0.1603, 0.2272, 0.3849]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'class_names': ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']}\n"
     ]
    }
   ],
   "source": [
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "from medclip import MedCLIPProcessor\n",
    "from medclip import PromptClassifier\n",
    "\n",
    "processor = MedCLIPProcessor()\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained()\n",
    "clf = PromptClassifier(model, ensemble=True)\n",
    "clf.cuda()\n",
    "\n",
    "# prepare input image\n",
    "from PIL import Image\n",
    "image = Image.open('./example_data/view1_frontal.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# prepare input prompt texts\n",
    "from medclip.prompts import generate_chexpert_class_prompts, process_class_prompts\n",
    "cls_prompts = process_class_prompts(generate_chexpert_class_prompts(n=10))\n",
    "inputs['prompt_inputs'] = cls_prompts\n",
    "\n",
    "# make classification\n",
    "output = clf(**inputs)\n",
    "print(output)\n",
    "# {'logits': tensor([[0.5154, 0.4119, 0.2831, 0.2441, 0.4588]], device='cuda:0',\n",
    "#       grad_fn=<StackBackward0>), 'class_names': ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medclip-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
